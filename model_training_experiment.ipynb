{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c80a35b",
   "metadata": {},
   "source": [
    "# Image Classification Experimantation Notebook\n",
    "\n",
    "This is not a production notebook. However, it can be sufficiently used to:\n",
    "\n",
    "1. Download image datasets from a certain URL\n",
    "2. Visualize part of the set\n",
    "3. Retrain a premade Neural Network from Keras Applications with pre-trained weights\n",
    "4. Graph metrics during training\n",
    "5. Calculate and plot the confusion matrix\n",
    "6. Save the trained model\n",
    "7. Save an optimized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46852db4-4e13-451a-8413-084866ddc3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import matplotlib.image as mpimg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0644843-6872-4a05-9370-b2791ade6c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# directories\n",
    "CWD = os.getcwd() # current working directory\n",
    "IMAGE_DATA = os.path.join(CWD,'data') # data directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3b78af-7d08-4cff-b62b-1cf2151fa65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define image classes' names\n",
    "classes = [x for x in os.listdir(IMAGE_DATA) if os.path.isdir(os.path.join(IMAGE_DATA,x))]\n",
    "num_classes = len(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263d70f0-56c0-471e-9ad6-a4c6f916e1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display sample image data set\n",
    "num_images = 3  # number of images per class\n",
    "image_size = 3  # size of the displayed images in inches\n",
    "fig, ax = plt.subplots(nrows=len(classes), ncols=num_images, figsize=(image_size * num_images, image_size * len(classes)))\n",
    "\n",
    "for i, c in enumerate(classes):\n",
    "    class_dir = os.path.join(IMAGE_DATA, c)\n",
    "    class_files = os.listdir(class_dir)\n",
    "    random_files = random.sample(class_files, num_images)\n",
    "    \n",
    "    for j, f in enumerate(random_files):\n",
    "        img = mpimg.imread(os.path.join(class_dir, f))\n",
    "        ax[i][j].imshow(img)\n",
    "        ax[i][j].set_title(c)\n",
    "        ax[i][j].set_xticks([])\n",
    "        ax[i][j].set_yticks([])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd3da79",
   "metadata": {},
   "source": [
    "## Image Data Generators for Training and Validation\n",
    "\n",
    "Below, we define the image data generators for training and validation datasets.\n",
    "\n",
    "The code uses the `ImageDataGenerator` class from the `tf.keras.preprocessing.image` module to create generators that read and preprocess images from a directory on the disk.\n",
    "\n",
    "### Parameters\n",
    "\n",
    "- `IMAGE_SIZE`: an integer that specifies the size of the images to be used as input to the model. The images are resized to this size during preprocessing.\n",
    "- `BATCH_SIZE`: an integer that specifies the number of images to be loaded and processed together in each batch during training and validation.\n",
    "- `SCALE`: an integer or float that specifies the factor by which the image pixel values are to be rescaled. In this case, the pixel values are divided by 255 to bring them into the range of 0 to 1.\n",
    "- `validation_split`: a float that specifies the fraction of images to be used for validation. In this case, 20% of the images are used for validation.\n",
    "- `subset`: a string that specifies whether the generator should be used for the 'training' or 'validation' dataset.\n",
    "\n",
    "The generators, `train_generator` & `val_generator` can then be used as input to a machine learning model that requires image data as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152e41a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 224\n",
    "BATCH_SIZE = 64\n",
    "SCALE = 255\n",
    "\n",
    "datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale=1./SCALE, \n",
    "    validation_split=0.2)\n",
    "\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    IMAGE_DATA,\n",
    "    target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "    batch_size=BATCH_SIZE, \n",
    "    subset='training')\n",
    "\n",
    "val_generator = datagen.flow_from_directory(\n",
    "    IMAGE_DATA,\n",
    "    target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "    batch_size=BATCH_SIZE, \n",
    "    subset='validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14601686",
   "metadata": {},
   "source": [
    "## Extract a batch of images and labels from the val_generator generator\n",
    "\n",
    "- `image_batch`: a tensor of shape `(batch_size, height, width, channels)` that contains a batch of images loaded from the generator.\n",
    "- `label_batch`: a tensor of shape `(batch_size, num_classes)` that contains the corresponding labels for each image in the image_batch.\n",
    "\n",
    "The `next()` method is used to extract a single batch of images and labels from the `val_generator` generator. The size of the batch is specified by the `batch_size` parameter used when creating the generator.\n",
    "\n",
    "The resulting `image_batch` and `label_batch` tensors are then printed to the console using the `print()` function. The image_batch tensor has a shape of `(batch_size, height, width, channels)` where `height` and `width` correspond to the dimensions of each image in the batch, and `channels` corresponds to the number of color channels in the images (e.g., 3 for RGB images). The `label_batch` tensor has a shape of `(batch_size, num_classes)` where `num_classes` is the number of classes in the dataset, and each row corresponds to the one-hot encoded label for each image in the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e37bfd-e91e-4980-9fc9-731a831f05b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_batch, label_batch = next(val_generator)\n",
    "image_batch.shape, label_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14bf810f",
   "metadata": {},
   "source": [
    "## Extract a Batch of Images and Labels from a Training Generator\n",
    "\n",
    "Here we extract a batch of images and labels from the `train_generator` generator.\n",
    "\n",
    "### Parameters\n",
    "- `x_train`: a tensor of shape `(batch_size, height, width, channels)` that contains a batch of images loaded from the generator.\n",
    "- `y_train`: a tensor of shape `(batch_size, num_classes)` that contains the corresponding labels for each image in the x_train tensor.\n",
    "\n",
    "The `next()` method is used to extract a single batch of images and labels from the `train_generator` generator. The size of the batch is specified by the `batch_size` parameter used when creating the generator.\n",
    "\n",
    "The resulting `x_train` and `y_train` tensors contain the batch of images and labels, respectively, and can be used to train a machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27270a2a-2c0b-40da-ba41-ea39401e3c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = next(train_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1632b7fb-6b85-4dfe-bc9c-e4c7a1dcd0e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c43cff-9984-41d8-9f25-c0974c6375fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the class indices and store them in the filesystem\n",
    "print(train_generator.class_indices)\n",
    "\n",
    "labels = '\\n'.join(sorted(train_generator.class_indices.keys()))\n",
    "\n",
    "with open(os.path.join(CWD,'image_labels.txt'), 'w') as f:\n",
    "  f.write(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c786d4",
   "metadata": {},
   "source": [
    "## Create a Base Model using the Pre-Trained MobileNet V2\n",
    "\n",
    "### Parameters\n",
    "\n",
    "- `IMG_SHAPE`: a tuple that specifies the expected shape of the input image, which is `(IMAGE_SIZE, IMAGE_SIZE, 3)`. The first two values correspond to the height and width of the image, while the third value corresponds to the number of channels (e.g., 3 for RGB images).\n",
    "- `input_shape`: a tuple that specifies the expected shape of the input tensor for the base model. In this case, it is set to `IMG_SHAPE`.\n",
    "- `include_top`: a boolean value that specifies whether to include the fully connected layer at the top of the network. In this case, it is set to False because the top layer will be replaced by a new one that is specific to the current problem.\n",
    "- `weights`: a string that specifies the pre-trained weights to use for the base model. In this case, it is set to 'imagenet' to use the weights trained on the ImageNet dataset.\n",
    "- `base_model`: the pre-trained MobileNet V2 model that will serve as the base model for transfer learning.\n",
    "- `base_model.trainable`: a boolean value that specifies whether the weights of the base model should be trainable during the fine-tuning process. In this case, it is set to False to freeze the weights of the pre-trained model.\n",
    "\n",
    "The trainable attribute of the `base_model` object is set to `False` to freeze the weights of the pre-trained model. This is done to prevent the weights from being updated during the training process, and to ensure that the pre-trained features are preserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c172e73-e159-42aa-97ac-49d2f2e6b1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SHAPE = (IMAGE_SIZE, IMAGE_SIZE, 3)\n",
    "\n",
    "# Create the base model from the pre-trained MobileNet V2\n",
    "base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,\n",
    "                                              include_top=False, \n",
    "                                              weights='imagenet')\n",
    "base_model.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f35dc3",
   "metadata": {},
   "source": [
    "## Create custom model using Transfer Learning\n",
    "\n",
    "The following code creates a custom model for classification using transfer learning with the MobileNet V2 base model.\n",
    "\n",
    "### Parameters\n",
    "\n",
    "- `base_model`: the pre-trained MobileNet V2 model that serves as the base model for transfer learning.\n",
    "- `tf.keras.Sequential()`: a function that creates a sequential model, which is a linear stack of layers.\n",
    "- `tf.keras.layers.Conv2D()`: a convolutional layer that performs convolution operations on the input data. The filters parameter specifies the number of output filters in the layer, the kernel_size parameter specifies the size of the convolution kernel, and the activation parameter specifies the activation function to use.\n",
    "- `tf.keras.layers.Dropout()`: a regularization layer that randomly drops some of the input units to prevent overfitting. The 0.2 parameter specifies the fraction of the input units to drop.\n",
    "- `tf.keras.layers.GlobalAveragePooling2D()`: a pooling layer that performs global average pooling on the input data. This reduces the spatial dimensions of the input and produces a feature vector that can be used as input to the final dense layer.\n",
    "- `tf.keras.layers.Dense()`: a fully connected layer that produces the final output of the model. The units parameter specifies the number of output units, which is set to `num_classes`. The activation parameter specifies the activation function to use, which is 'softmax' for multi-class classification.\n",
    "\n",
    "The `Conv2D` layer applies a convolution operation to the input data with 32 output filters, a kernel size of 3, and the ReLU activation function. The `Dropout` layer randomly drops 20% of the input units to prevent overfitting. The `GlobalAveragePooling2D` layer performs global average pooling on the output of the previous layer to produce a feature vector that is fed into the final Dense layer.\n",
    "\n",
    "The `Dense` layer produces the final output of the model, which is a probability distribution over the two classes. The units parameter is set to `num_classes` to output a probability for each of the classes, and the activation parameter is set to 'softmax' to ensure that the output values are normalized and sum to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0bb4f1b-eeec-47a5-892e-96e78617e690",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "  base_model,\n",
    "  tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.GlobalAveragePooling2D(),\n",
    "  tf.keras.layers.Dense(units=num_classes, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed560258",
   "metadata": {},
   "source": [
    "## Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23412fb2-4942-45e6-be4e-a58d3661a9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ca4e6b-2183-4883-a246-b6850806165f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba2d652-eb49-4d59-8c59-39e5daaaab80",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of trainable weights = {}'.format(len(model.trainable_weights)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b6f80a-a74d-4cf8-a4cb-e221665ea091",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_generator,\n",
    "                    steps_per_epoch=len(train_generator), \n",
    "                    epochs=5,\n",
    "                    validation_data=val_generator,\n",
    "                    validation_steps=len(val_generator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba99ee4-a651-4999-9dd6-50b53e678b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([min(plt.ylim()),1])\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('Cross Entropy')\n",
    "plt.ylim([0,1.0])\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2bf672-bbba-4e62-9188-2756d79a40f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965e5e83-416d-4cec-a7ce-d4e92bd2c577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the model to make predictions on the validation set\n",
    "Y_pred = model.predict_generator(val_generator, val_generator.n // BATCH_SIZE+1)\n",
    "y_pred = np.argmax(Y_pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b429996a-fadb-488e-a241-c7281f075d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the true labels of the validation set\n",
    "y_true = val_generator.classes\n",
    "\n",
    "# Calculate and plot the confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred, normalize='true')\n",
    "classes = val_generator.class_indices.keys()\n",
    "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.title('Confusion matrix')\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(len(classes))\n",
    "plt.xticks(tick_marks, classes, rotation=45)\n",
    "plt.yticks(tick_marks, classes)\n",
    "fmt = '.2f'\n",
    "thresh = cm.max() / 2.\n",
    "for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "    plt.text(j, i, format(cm[i, j], fmt),\n",
    "             horizontalalignment=\"center\",\n",
    "             color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "plt.tight_layout()\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba1cdf3-22e2-4861-9db9-a04132c42925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "model.save('flower_classification_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9658a58c-99d6-452c-8e06-625901e13db9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8fba77-5734-420e-afbf-4c2dc612f05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "with open('mobilenet_v2_1.0_224.tflite', 'wb') as f:\n",
    "  f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc944bf-03e7-4463-a2c7-88f08b41ccfe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quality_inspection",
   "language": "python",
   "name": "quality_inspection"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
